PRACTICAL 1:
AIM : Document Indexing and Retrieval.
● Implement an inverted index construction algorithm.
● Build a simple document retrieval system using the constructed index.

CODE: 
document1 = "The quick brown fox jumped over the lazy dog"
document2 = "The lazy dog slept in the sun"

token1 = document1.lower().split()
token2 = document2.lower().split()

terms = list(set(token1 + token2))

inverted_index = {}

for term in terms:
    documents = []
    if term in token1:
        documents.append("document1")
        if term in token2:
            documents.append("document2")
            inverted_index[term] = documents

for term, documents in inverted_index.items():
    print(term, "->",",".join(documents))





PRACTICAL 2: 
AIM: Retrieval Models:
● Implement the Boolean retrieval model and process queries.
● Implement the vector space model with TF-IDF weighting and cosine similarity.

1. Boolean Retrieval Model

CODE: 
documents = {
    1: "apple banana orange",
    2: "apple banana",
    3: "banana orange",
    4: "apple"
}

def build_index(docs):
    index = {}  
    for doc_id, text in docs.items():  
        terms = set(text.split()) 
        for term in terms: 
            if term not in index:
                index[term] = {doc_id} 
            else:
                index[term].add(doc_id)  
    return index  

inverted_index = build_index(documents)

def boolean_and(operands, index):
    if not operands:  
        return list(range(1, len(documents) + 1))

    result = index.get(operands[0], set()) 
    for term in operands[1:]: 
        result = result.intersection(index.get(term, set()))  
    return list(result)  

def boolean_or(operands, index):
    result = set()  
    for term in operands:  
        result = result.union(index.get(term, set()))  
    return list(result)  

def boolean_not(operand, index, total_docs):
    operand_set = set(index.get(operand, set()))  
    all_docs_set = set(range(1, total_docs + 1))  
    return list(all_docs_set.difference(operand_set))


query1 = ["apple", "banana"]  
query2 = ["apple", "orange"]

result1 = boolean_and(query1, inverted_index)  
result2 = boolean_or(query2, inverted_index)  
result3 = boolean_not("orange", inverted_index, len(documents)) 

print("Documents containing 'apple' and 'banana':", result1)
print("Documents containing 'apple' or 'orange':", result2)
print("Documents not containing 'orange':", result3)




2. TF-IDF weighting and Cosine Similarity

CODE: 
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  
import nltk 
from nltk.corpus import stopwords  
import numpy as np 
from numpy.linalg import norm  

train_set = ["The sky is blue.", "The sun is bright."]  
test_set = ["The sun in the sky is bright."]  

nltk.download('stopwords')
stopWords = stopwords.words('english')

vectorizer = CountVectorizer(stop_words=stopWords)  
transformer = TfidfTransformer()  

trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()  
testVectorizerArray = vectorizer.transform(test_set).toarray()  

print('Fit Vectorizer to train set', trainVectorizerArray)
print('Transform Vectorizer to test set', testVectorizerArray)

cx = lambda a, b: round(np.inner(a, b) / (norm(a) * norm(b)), 3)

for vector in trainVectorizerArray:
    print(vector)
    for testV in testVectorizerArray:
        print(testV)  
        cosine = cx(vector, testV)
        print(cosine)  

transformer.fit(trainVectorizerArray)
print()
print(transformer.transform(trainVectorizerArray).toarray())

transformer.fit(testVectorizerArray)
print()
tfidf = transformer.transform(testVectorizerArray)
print(tfidf.todense())





PRACTICAL 3:
AIM: Spelling Correction in IR systems
● Develop a spelling correction module using edit distance algorithms.
● Integrate the spelling correction module into an information retrieval system.

CODE: 
def editDistance(str1, str2,m,n):
    if m == 0:
        return n
    if n == 0:
        return m
    if str1[m-1] == str2[n-1]:
        return editDistance(str1,str2,m-1,n-1)
    return 1 + min(editDistance(str1, str2, m, n-1),
                   editDistance(str1, str2, m-1, n),
                   editDistance(str1, str2, m-1, n-1),)

str1 = "Sunday"
str2 = "Saturday"
print("Edit Distance is : ", editDistance(str1, str2, len(str1), len(str2)))





PRACTICAL 4 : 
AIM: Evaluation Metrics for IR Systems.
● Calculate precision, recall, and F-measure for a given set of retrieval results.
● Use an evaluation toolkit to measure average precision and other evaluation metrics.
 
1. Calculate Precision, Recall and F-measure:

CODE: 
def calculate_matrics(retrived_set, relevant_set):
    true_positive = len(retrived_set.intersection(relevant_set))
    false_positive = len(retrived_set.difference(relevant_set))
    false_negetive = len(relevant_set.difference(retrived_set))


    print("True Positive: ", true_positive,
          "\nFalse Positive: ", false_positive,
          "\nFalse Negetive: ", false_negetive, "\n")

    precision = true_positive/(true_positive + false_positive)
    recall = true_positive/(true_positive + false_negetive)
    f_measure = 2*precision*recall/(precision+recall)

    return precision, recall, f_measure

retrived_set = set(["doc1", "doc2", "doc3"])
relevant_set = set(["doc1", "doc2"])

precision,recall,f_measure = calculate_matrics(retrived_set, relevant_set)

print(f"Precision: , {precision}")
print(f"Recall: , {recall}")
print(f"F-measure: , {f_measure}")




2. Using an evaluation toolkit to measure average precision and other evaluation metrics.

CODE: 
from sklearn.metrics import average_precision_score

y_true = [0,1,1,0,1,1]
y_score = [0.1,0.4,0.35,0.8,0.65,0.9]

average_precision = average_precision_score(y_true,y_score)

print(f'Average Precision-recall scores: , {average_precision}') 





PRACTICAL 5:  
AIM: Text Categorization:
● Implement a text classification algorithm (e.g., Naive Bayes or Support Vector 
Machines).
● Train the classifier on a labeled dataset and evaluate its performance.
(Dataset.csv:, Test.csv:)


Make note you have a two datasets :
1. Dataset.csv
2. Test.csv

CODE: 
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

df = pd.read_csv(r"C:\\Users\\Raj\\OneDrive\\Documents\\Practicals\\IR\\Dataset.csv")
data=df["covid"]+" "+df["fever"]
X = data.astype(str)
y = df['flu']  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

vectorizer = CountVectorizer()                       
X_train_counts = vectorizer.fit_transform(X_train)   
X_test_counts = vectorizer.transform(X_test)        

classifier = MultinomialNB()                
classifier.fit(X_train_counts, y_train) 

data1 = pd.read_csv(r"C:\\Users\\Raj\\OneDrive\\Documents\\Practicals\\IR\\Test.csv")
new_data=data1["covid"]+" "+data1["fever"]
new_data_counts = vectorizer.transform(new_data.astype(str)) 

predictions = classifier.predict(new_data_counts)

new_data = predictions
print(new_data)

accuracy = accuracy_score(y_test, classifier.predict(X_test_counts))
print(f"\nAccuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, classifier.predict(X_test_counts)))
predictions_df = pd.DataFrame(predictions, columns=['flu_prediction'])
data1 = pd.concat([data1, predictions_df], axis=1)
data1.to_csv(r"C:\\Users\\Raj\\OneDrive\\Documents\\Practicals\\IR\\Test.csv", index=False)





PRACTICAL 6 : 
AIM: Clustering for Information Retrieval
● Implement a clustering algorithm (e.g., K-means or hierarchical clustering).
● Apply the clustering algorithm to a set of documents and evaluate the clustering results.

CODE: 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

documents = ["Cats are known for their agility and grace",
             "Dogs are often called ‘man’s best friend",
             "Some dogs are trained to assist people with disabilities",
             "The sun rises in the east and sets in the west"
              "Many cats enjoy climbing trees and chasing toys.",]

vectorizer = TfidfVectorizer(stop_words = 'english')

X = vectorizer.fit_transform(documents)

kmeans = KMeans(n_clusters = 3, random_state = 0).fit(X)

print(kmeans.labels_)





PRACTICAL 7: 
AIM: Web Crawling and Indexing
● Develop a web crawler to fetch and index web pages.
● Handle challenges such as robots.txt, dynamic content, and crawling delays.


CODE: 
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser

def get_html(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f"HTTP Error: {errh}")
    except requests.exceptions.RequestException as err:
        print(f"Request Error: {err}")
    return None

def save_robots_txt(url):
    try:
        robots_url = urljoin(url, '/robots.txt')
        robots_content = get_html(robots_url)
        if robots_content:
            with open('robots.txt', 'wb') as file:
                file.write(robots_content.encode('utf-8-sig'))
    except Exception as e:
        print(f"Error saving robots.txt: {e}")

def load_robots_txt():
    try:
        with open('robots.txt', 'rb') as file:
            return file.read().decode('utf-8-sig')
    except FileNotFoundError:
        return None

def extract_links(html, base_url):
    soup = BeautifulSoup(html, 'html.parser')
    links = []
    for link in soup.find_all('a', href=True):
        absolute_url = urljoin(base_url, link['href'])
        links.append(absolute_url)
    return links

def is_allowed_by_robots(url, robots_content):
    parser = RobotFileParser()
    parser.parse(robots_content.split('\n'))
    return parser.can_fetch('*', url)

def crawl(start_url, max_depth=3, delay=1):
    visited_urls = set()

    def recursive_crawl(url, depth, robots_content):
        if depth > max_depth or url in visited_urls or not is_allowed_by_robots(url, robots_content):
            return
        visited_urls.add(url)
        time.sleep(delay)

        html = get_html(url)
        if html:
            print(f"Crawling {url}")
            links = extract_links(html, url)

            for link in links:
                recursive_crawl(link, depth + 1, robots_content)

    save_robots_txt(start_url)
    robots_content = load_robots_txt()
    if not robots_content:
        print("Unable to retrieve robots.txt. Crawling without restrictions.")
    
    recursive_crawl(start_url, 1, robots_content)

crawl('https://wikipedia.com', max_depth=2, delay=2)







PRACTICAL 8 : 
AIM: Link Analysis and PageRank
● Implement the PageRank algorithm to rank web pages based on link analysis.
● Apply the PageRank algorithm to a small web graph and analyze the results.



CODE:
import numpy as np

def page_rank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
    num_nodes = len(graph)
    page_ranks = np.ones(num_nodes) / num_nodes

    for _ in range(max_iterations):
        prev_page_ranks = np.copy(page_ranks)

        for node in range(num_nodes):
            incoming_links = [i for i, v in enumerate(graph) if node in v]
            if not incoming_links:
                continue

            page_ranks[node] = (1 - damping_factor) / num_nodes + \
                               damping_factor * sum(prev_page_ranks[link] / len(graph[link]) for link in incoming_links)

        if np.linalg.norm(page_ranks - prev_page_ranks, 2) < tolerance:
            break

    return page_ranks

if __name__ == "__main__":
    web_graph = [
        [1, 2],    
        [0, 2],     
        [0, 1] , 
        [1,2]
    ]

    result = page_rank(web_graph)

    for i, pr in enumerate(result):
        print(f"Page {i}: {pr}")






PRACTICAL 9: 
AIM: Learning to Rank 
 ● Implement a learning to rank algorithm (e.g., RankSVM or RankBoost). 
 ● Train the ranking model using labeled data and evaluate its effectiveness. 


CODE: 
from sklearn import svm, model_selection
import numpy as np

x = np.array([[1,1],[1,2],[1,3],[1,1],[1,1],[1,1],[1,3],[1,2]])

y = np.array(["High Relevance","Low Relevance", "Irrelevant","High Relevance",
              "High Relevance","High Relevance","Irrelevant","Low Relevance"])

model = svm.SVC(kernel='linear')

scores = model_selection.cross_val_score(model, x, y, cv=2)

model.fit(x,y)

new_data = np.array([[1,3],[1,1],[1,2]])
prediction = model.predict(new_data)
print(f'model : , {model}')
print(f'new data : , {new_data}')
print(f'Prediction for new data : , {prediction}')





PRACTICAL 10 : 
AIM: Advanced Topics in Information Retrieval 
 ● Implement a text summarization algorithm (e.g., extractive or abstractive). 
 ● Build a question-answering system using techniques such as information extraction.

CODE: 
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def extractive_summarization_and_qa(text, question=None, num_sentences=3):
    def extractive_summarization(text, num_sentences):
        sentences = nltk.sent_tokenize(text)
        tfidf_vectorizer = TfidfVectorizer()
        tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)
        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
        scores = similarity_matrix.sum(axis=1)
        ranked_sentences_indices = scores.argsort()[-num_sentences:][::-1]
        summary = [sentences[i] for i in sorted(ranked_sentences_indices)]
        return ' '.join(summary)

    def answer_question(question, text):
        sentences = nltk.sent_tokenize(text)
        sentences.insert(0, question)
        tfidf_vectorizer = TfidfVectorizer()
        tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)
        similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)
        index = similarity_matrix.argsort()[0][-2]
        return sentences[index]

    if question:
        return answer_question(question, text)
    else:
        return extractive_summarization(text, num_sentences)

text = "Scientists recently discovered a new species of butterfly in the Amazon rainforest. The butterfly, named Morpho amadeus, has vibrant blue wings with intricate patterns. Researchers believe its discovery sheds light on the biodiversity of the region and underscores the importance of conservation efforts in preserving fragile ecosystems like the Amazon."
question = "What was recently discovered in the Amazon rainforest?"

result = extractive_summarization_and_qa(text, question)
print(result)

